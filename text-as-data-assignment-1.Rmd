---
title: 'Text as data: Assignment 1'
output:
  html_document: default
  pdf_document: default
date: '2022-10-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)
library(quanteda)
library(quanteda.textstats)

```

## Getting and parsing texts

To start with, you are asked to retrieve *Songs of Innocence and of Experience* by William Blake from Project Gutenberg. It is located at https://www.gutenberg.org/cache/epub/1934/pg1934.txt. This is a collection of poems in two books: *Songs of Innocence* and *Songs of Experience*.

Parse this into a dataframe where each row is a line of a poem (there should be no empty lines). The following columns should describe where each line was found:

- line_number 
- stanza_number
- poem_title
- book_title

```{r}
#we start by getting the file from where it's located
blake <- read_file("https://www.gutenberg.org/cache/epub/1934/pg1934.txt")

```

```{r}
#first, we find the names of the books
pattern_songs <- "([A-Z ]+)\r\n\r\n\r\n\r\n\r\nINTRODUCTION"
songs <- str_match_all(blake, pattern_songs)[[1]][,2]
songs

```
```{r}
#then, we find the complete text of the books
pattern_innocence <- "(?<=SONGS OF INNOCENCE[\\r\\n]{5})([\\s\\S]*)(?=SONGS OF EXPERIENCE)"
pattern_experience <- "(?<=SONGS OF EXPERIENCE[\\r\\n]{5})([\\s\\S]*)(?=[\\r\\n]{5}+[\\*{3}])"

innocence <- str_match_all(blake, pattern_innocence)[[1]][,1]
experience <- str_match_all(blake, pattern_experience)[[1]][,1]

```

```{r}
#we create a data frame for the text, then create a row for each poem
blake_df_01 <- data.frame(book_title = songs, complete_text = c(innocence, experience)) %>% mutate(complete_text = strsplit(as.character(complete_text), "\r\n\r\n\r\n\r\n\r\n")) %>% unnest(complete_text)
blake_df_01

#we create a column for poem titles
blake_df_01 <- blake_df_01 %>% separate(complete_text, into = c("poem_title", "stanzas"), sep = "\r\n\r\n\r\n")
blake_df_01

```

```{r}
#then, we separate the stanzas and number them
blake_df_02 <- blake_df_01 %>% mutate(stanzas = strsplit(as.character(stanzas), "\r\n\r\n"))  %>% unnest(stanzas) %>% group_by(book_title,poem_title) %>% mutate(stanza_number = 1:n())
blake_df_02

```

```{r}
#similarly, we separate the lines and number them
blake_df_02 <- blake_df_02 %>% mutate(stanzas = strsplit(as.character(stanzas), "\r\n")) %>% unnest(stanzas) %>% group_by(book_title, poem_title) %>% mutate(line_number = 1:n())
blake_df_02

```

```{r}
#then we clean the data frame
blake_df <- rename(blake_df_02, lines = stanzas)
blake_df <- blake_df[, c("lines", "book_title", "poem_title", "stanza_number", "line_number")]
blake_df$poem_title <- gsub("\n\r\n\r\n", "", blake_df$poem_title)
blake_df$poem_title <- gsub("\r\n", "", blake_df$poem_title)

#and finally, we print the clean, final version
blake_df

```

## Visualising text data

- Create a histogram showing the number of lines per poem
- Create a document feature matrix treating each line as a document
- Create a separate document feature matrix treating each poem as a document
- Using one of these document feature matrices, create a plot that compares the frequency of words in each book. Comment on the features that are more or less frequent in one book than another.

```{r}
#we prepare the data for the histogram
hist_df <- blake_df %>% group_by(poem_title) %>% summarise(max = max(line_number, na.rm=TRUE))
hist_df

#we create the histogram using ggplot
plot01 <- ggplot(hist_df, aes(x=max)) +
  geom_histogram(binwidth=4) +
  labs(title = "Lines per Poem", x = "Number of Lines", y = "Count") +
  theme_bw()
plot01

```
```{r}
#we create a document feature matrix, treating each line as a document
lines_matrix <- blake_df$lines %>% tokens(remove_punc=TRUE) %>% tokens_remove(pattern=stopwords("en")) %>% tokens_wordstem("english") %>% dfm()
lines_matrix

```
```{r}
#we rearrange an earlier version of the data frame from section 1 to treat each poem as a document
blake_df_01$poem_title <- gsub("\n\r\n\r\n", "", blake_df_01$poem_title)
blake_df_01$poem_title <- gsub("\r\n", "", blake_df_01$poem_title)
blake_df_01$stanzas <- gsub("\r\n", " ", blake_df_01$stanzas)
blake_df_01

#we create a document feature matrix, treating each poem as a document
poem_matrix <- blake_df_01$poem_title %>% tokens(remove_punc=TRUE) %>% tokens_remove(pattern=stopwords("en")) %>% tokens_wordstem("english") %>% dfm()
poem_matrix

```
```{r}
#we create a freq table for all features
lines_freq <- lines_matrix %>% textstat_frequency(groups=blake_df$book_title)
lines_freq %>% head()

#then, we filter by group, and create freq tables for both books' top features
in_freq <- lines_freq %>% filter(group == "SONGS OF INNOCENCE") %>% head(20)
ex_freq <- lines_freq %>% filter(group == "SONGS OF EXPERIENCE") %>% head(20)
top_freq <- bind_rows(in_freq, ex_freq)
top_freq %>% head()
```

We can create 2 separate frequency table to observe how top features change:

```{r}
#we create plots to explore both tables
plot_02 <- ggplot(in_freq, aes(y=reorder(feature, frequency), x=frequency)) +
  geom_col() +
  labs(title = "Top 20 Features in Songs of Innocence", x = "Features", y = "Count") +
  theme_bw()
plot_02

plot_03 <- ggplot(ex_freq, aes(y=reorder(feature, frequency), x=frequency)) +
  geom_col() +
  labs(title = "Top 20 Features in Songs of Experience", x = "Features", y = "Count") +
  theme_bw()
plot_03

```

We observe from these top features plots that the books are thematically differ from one another. Songs of Innocence has a much more nostalgic and melancholic tone, drawing on childhood (note features such as sweet, little, lamb, child, infant, and father) and pure emotions (note features such as joy, weep, laugh, happi, and smile). On the other hand, Song of Experience has a much more darker tone (note features such as night, fear, morn, human, and away). There are 9 features that appear in both lists (thi, love, weep, sleep, littl, can, joy, father, sweet), where 4 of them appear more on Songs of Innocence (littl, joy, sweet, can), and 4 of them appear more on Songs of Experience (thi, love, weep, sleep), 1 of them appear an exact same amount (father). 


## Parsing XML text data

Now we will work with German Parliamentary data, which is available in XML format [here](https://www.bundestag.de/services/opendata) for the last two parliamentary periods. Remember XML format is very like HTML format, and we can parse it using a scraper and CSS selectors. Speeches are contained in `<rede>` elements, which each contain a paragraph element describing the speaker, and paragraph elements recording what they said. Not that class selectors won't work, because the class attribute is called "klasse". You can use normal attribute selectors.

Choose one of the sessions, and retrieve it using R or Python. Using a scraper, get a list of all the <rede> elements. For each element, get the name of the speaker, and a single string containing everything that they said. Put this into a dataframe. Print the number of speeches, and the content of the first speech, by a politician of your choice.

```{r}
#first, we read the xml text data
library(rvest)
link <- read_html("https://www.bundestag.de/resource/blob/901386/2a11260627e1e60eb570abdcaf1d4c60/20045-data.xml")
link

```

```{r}
#we find the relevant node set
speeches <- link %>% html_elements("rede")
speeches

```


```{r}
#we create an empty data frame
plenaryinfo = data.frame()

#then we put each node in a loop to get the relevant info
for (speech in speeches) {
  
  #we get speaker name, lastname, and speech text
  speaker_name <- speech %>% html_elements("p redner name vorname") %>% html_text2()
  speaker_lastname <- speech %>% html_elements("p redner name nachname") %>% html_text2()
  speechtext <- speech %>% html_elements("p[klasse!=redner]") %>% html_text2() %>% paste(collapse = " ")

  #we put the info in the data frame
  plenary = c(speaker_name, speaker_lastname, speechtext)
  plenaryinfo = rbind(plenaryinfo, plenary)
  }

#we rename the columns appropriately
colnames(plenaryinfo)<-c("speaker_name", "speaker_lastname", "speech_text")

#and print
plenaryinfo %>% head()

```

Let's find the number of speeches by Katrin Helling-Plahr:
```{r}
#we filter by last name
filter_hellingplahr <- filter(plenaryinfo, speaker_lastname == "Helling-Plahr")
filter_hellingplahr

#then print the number of rows to get number of speeches
filter_hellingplahr %>% nrow()

```

We can get the speech of the first speaker by calling the relevant row and column:
```{r}
#we call on the first speech
filter_hellingplahr[1, 3]
```

## Using regular expressions

Using a regular expression, get a list of words spoken in your parliamentary protocol that contain (in upper or lower case) the string "kohle" (coal). Show the number of occurrences of each of these words. If there are no mentions in the debate you have selected, try another protocol.

```{r}
#we create a pattern for "kohle" with regex
pattern_kohle <- "\b?[Kk]ohle\b?"

#then we match all strings in the speech_text column of the data frame
str_match_all(plenaryinfo, pattern_kohle)[[3]][,1] 

```

